{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wquantiles\n",
    "import matplotlib\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import bootstrapped.bootstrap as bs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's be fancy!\n",
    "from IPython.core.display import HTML\n",
    "HTML('<link href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T\" crossorigin=\"anonymous\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"p-3 mb-2 bg-primary text-white\">1. Loading the data set</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(r\"data/training_dataset.csv\", sep=\";\")\n",
    "Y_train = pd.read_csv(r\"data/training_solution.csv\", sep=\";\", names=X_train.columns)\n",
    "\n",
    "X_test = pd.read_csv(r\"data/test_dataset.csv\", sep=\";\")\n",
    "\n",
    "assert np.array_equal(X_train.columns, X_test.columns)\n",
    "COLS = X_train.columns.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"p-3 mb-2 bg-primary text-white\">2. Explorative Data Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">RQ: How can the data be described?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()  # train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.sum(axis=0)  # number of failure labelings per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()  # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">RQ: Is the test data sampled from the same distribution?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare train and test histograms\n",
    "for col in COLS:\n",
    "    plt.figure(figsize=(25, 5))\n",
    "\n",
    "    # (1) histogram\n",
    "    plt.subplot(1, 2, 1, label=\"histogram\")\n",
    "\n",
    "    bins = np.histogram_bin_edges(np.concatenate([X_train[col], X_test[col]]), bins=50)\n",
    "    plt.hist(X_train[col], alpha=0.5, bins=bins, label=\"train\")\n",
    "    plt.hist(X_test[col], alpha=0.5, bins=bins, label=\"test\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"{} histogram\".format(col))\n",
    "\n",
    "    # (2) raw values\n",
    "    plt.subplot(1, 2, 2, label=\"raw values\")\n",
    "\n",
    "    plt.plot(X_train[col], label=\"train\")        \n",
    "    plt.plot(X_test[col], label=\"test\")\n",
    "\n",
    "    errorband = np.where(Y_train[col])[0]\n",
    "    if len(errorband) > 0:\n",
    "        plt.fill_between(errorband, np.zeros_like(errorband), X_train[col][errorband],\n",
    "                         facecolor='yellow', alpha=0.5, label=\"failures\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"{} raw values\".format(col))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">RQ: Are the features correlated?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(x, y=None, name=\"\"):\n",
    "    plt.figure(figsize=(20, 1.1 * x.shape[1]))\n",
    "    for i, col in enumerate(x.columns):\n",
    "        y_ = sklearn.preprocessing.minmax_scale(x[col]) + i + 0.5\n",
    "        plt.plot(y_, label=col)\n",
    "\n",
    "        if y is not None:\n",
    "            errorband = np.where(y[col])[0]\n",
    "            if len(errorband) > 0:\n",
    "                plt.fill_between(errorband, np.full_like(errorband, i + 0.5, dtype=np.float),\n",
    "                                 y_[errorband], facecolor='yellow', alpha=0.75)\n",
    "        \n",
    "    plt.gca().set_yticks(np.arange(1, len(x.columns) + 1))\n",
    "    plt.gca().set_xticks(np.arange(len(x), step=20))\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.grid(axis=\"x\")\n",
    "    plt.legend()\n",
    "    plt.title(\"feature correlation {}\".format(name))\n",
    "    plt.show()\n",
    "    \n",
    "visualize_features(X_train, Y_train, name=\"train\")\n",
    "visualize_features(X_test, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correlation_coefficients(x, name=\"\", method=\"spearman\"):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    im = plt.imshow(np.abs(x.corr(method)), cmap=\"viridis\")\n",
    "    \n",
    "    ticks = np.arange(0, len(x.columns))\n",
    "    labels = ticks + 1\n",
    "    plt.gca().set_xticks(ticks)\n",
    "    plt.gca().set_yticks(ticks)\n",
    "    plt.gca().set_xticklabels(labels)\n",
    "    plt.gca().set_yticklabels(labels)\n",
    "    \n",
    "    plt.title(\"absolute feature correlation {}\".format(name))\n",
    "    plt.colorbar(im)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_correlation_coefficients(X_train, \"train\")\n",
    "visualize_correlation_coefficients(X_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"p-3 mb-2 bg-primary text-white\">3. Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">Fix the erros in the training data set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_features_heuristically(x, y):\n",
    "    # fix constant offset in x2 by subtracting the mean\n",
    "    y2_errorband = np.where(y[\"x2\"])[0]\n",
    "    y2_faulty = x[\"x2\"][y2_errorband].values\n",
    "    x2_faulty = np.arange(len(y2_faulty)).reshape(-1, 1)\n",
    "    \n",
    "    y2_offset = np.mean(y2_faulty) - np.median(x[\"x2\"])\n",
    "    \n",
    "    y2_fixed = y2_faulty - y2_offset\n",
    "    \n",
    "    # fix constant trend in x3 by subtracting the linear component\n",
    "    y3_errorband = np.where(y[\"x3\"])[0]\n",
    "    y3_faulty = x[\"x3\"][y3_errorband].values\n",
    "    x3_faulty = np.arange(len(y3_faulty)).reshape(-1, 1)\n",
    "    \n",
    "    linreg = sklearn.linear_model.LinearRegression()\n",
    "    model = linreg.fit(x3_faulty, y3_faulty)\n",
    "    \n",
    "    y3_fixed = y3_faulty - (x3_faulty * model.coef_).ravel()\n",
    "    \n",
    "    # build the new data frame\n",
    "    result = x.copy()\n",
    "    result.loc[y2_errorband, \"x2\"] = y2_fixed\n",
    "    result.loc[y3_errorband, \"x3\"] = y3_fixed\n",
    "    \n",
    "    return result\n",
    "\n",
    "X_train_fixed = fix_features_heuristically(X_train, Y_train)\n",
    "\n",
    "visualize_features(pd.DataFrame(X_train.loc[:, [\"x2\", \"x3\"]]), Y_train, name=\"train (original x2 + x3)\")\n",
    "visualize_features(pd.DataFrame(X_train_fixed.loc[:, [\"x2\", \"x3\"]]), Y_train, name=\"train (fixed x2 + x3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">Correlation Confidence Intervals</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_confidence_interval(a, b, alpha=0.05, method=\"spearman\"):\n",
    "    # (c) http://onlinestatbook.com/2/estimation/correlation_ci.html\n",
    "    assert method == \"pearson\" or method == \"spearman\"\n",
    "    \n",
    "    n = len(a)\n",
    "    r, p = sp.stats.pearsonr(a, b) if method == \"pearson\" else sp.stats.spearmanr(a, b)\n",
    "    z_ = np.arctanh(r)  # transform r to fisher z score\n",
    "    \n",
    "    stderr = 1.0 / math.sqrt(n - 3)  # standard error or normally-distributed sampling distribution\n",
    "    z_crit = sp.stats.norm.ppf(1 - alpha / 2)  # 2-tailed z critical value\n",
    "\n",
    "    r_low = np.tanh(z_ - z_crit * stderr)\n",
    "    r_high = np.tanh(z_ + z_crit * stderr)\n",
    "    \n",
    "    return r_low, r_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"p-3 mb-2 bg-primary text-white\">4. Predictive Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(x_true, x_pred, x_pred_offset=0, name=\"\",\n",
    "                         error_inner=None, error_outer=None, error_lower=None, error_upper=None,\n",
    "                         y_mark=None, stretch=False):\n",
    "    plt.figure(figsize=(20 if not stretch else 60, 4))\n",
    "    \n",
    "    x_true_range = np.arange(len(x_true))\n",
    "    x_pred_range = np.arange(x_pred_offset, len(x_pred) + x_pred_offset)\n",
    "               \n",
    "    plt.plot(x_true_range, x_true, color=\"black\", alpha=0.5, ls=\"--\", label=\"true\")\n",
    "    plt.plot(x_pred_range, x_pred, color=\"orange\", label=\"prediction\")\n",
    "    \n",
    "    assert (error_inner is not None or error_outer is not None) ^ (error_lower is not None or error_upper is not None)\n",
    "\n",
    "    if error_inner is not None:\n",
    "        plt.fill_between(x_pred_range, x_pred - error_inner, x_pred + error_inner, color=\"orange\", alpha=0.2)\n",
    "    if error_outer is not None:\n",
    "        plt.fill_between(x_pred_range, x_pred - error_outer, x_pred + error_outer, color=\"orange\", alpha=0.2)\n",
    "    if error_lower is not None or error_upper is not None:\n",
    "        error_lower = error_lower if error_lower is not None else np.zeros_like(x_pred)\n",
    "        error_upper = error_upper if error_upper is not None else np.zeros_like(x_pred)\n",
    "        plt.fill_between(x_pred_range, x_pred - error_lower, x_pred + error_upper, color=\"orange\", alpha=0.2)\n",
    "\n",
    "    if y_mark is not None:\n",
    "        y_low, y_high = plt.ylim()\n",
    "        y_chunk = (y_high - y_low) / 50\n",
    "        plt.fill_between(x_pred_range, y_low, y_high, where=y_mark, facecolor='yellow', alpha=0.25, zorder=-1)\n",
    "        plt.fill_between(x_pred_range, y_high - y_chunk, y_high, where=y_mark, facecolor='red', alpha=0.5, zorder=-1)\n",
    "        plt.fill_between(x_pred_range, y_low, y_low + y_chunk, where=y_mark, facecolor='red', alpha=0.5, zorder=-1)\n",
    "                \n",
    "    if stretch:\n",
    "        minor = matplotlib.ticker.MultipleLocator(base=1)\n",
    "        plt.gca().xaxis.set_minor_locator(minor)\n",
    "        \n",
    "        major = matplotlib.ticker.MultipleLocator(base=5)\n",
    "        plt.gca().xaxis.set_major_locator(major)\n",
    "        \n",
    "        plt.grid(which=\"minor\", axis=\"x\", linestyle='--')\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        \n",
    "    plt.title(\"feature predictions {}\".format(name))\n",
    "    plt.xlim(x_pred_range[0], x_pred_range[-1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# visualize_prediction(X_test[\"x1\"], pred, error_lower=pred - err[:, 0], error_upper=err[:, 1] - pred, y_mark=mark, stretch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">Ridge Regression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(train, column, test=None, test_names=None, ridge_alpha=100, ci_alpha=0.01):\n",
    "    test = test if test is not None else train\n",
    "    test_names = test_names if test_names is not None else \"\"\n",
    "    if not isinstance(test, (list,)):\n",
    "        test = [test]\n",
    "    if not isinstance(test_names, (list,)):\n",
    "        test_names = [test_names]\n",
    "        \n",
    "    X, y = train.loc[:, COLS != column], train[column]\n",
    "    Z = np.column_stack([X, y])\n",
    "    \n",
    "    ridge = sklearn.linear_model.Ridge(alpha=ridge_alpha)\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler().fit(X, y)\n",
    "    model = ridge.fit(scaler.transform(X), y)\n",
    "    \n",
    "    def _ridge_predict_error(values):\n",
    "        X_, y_true = values[:, :-1], values[:, -1]\n",
    "        y_pred = model.predict(scaler.transform(X_))\n",
    "        return np.percentile(np.abs(y_true - y_pred), q=95)\n",
    "    \n",
    "    def _ridge_predict_error_bootstrap(samples):\n",
    "        if samples.ndim == 2:  # single observation\n",
    "            return [_ridge_predict_error(samples)]\n",
    "        return [_ridge_predict_error(samples[i, :, :]) for i in range(samples.shape[0])]\n",
    "    \n",
    "    # do bootstrap to get confidence interval\n",
    "    ci = bs.bootstrap(Z, stat_func=_ridge_predict_error_bootstrap, alpha=ci_alpha)\n",
    "    \n",
    "    # prediction\n",
    "    for t, n in zip(test, test_names):\n",
    "        X_, y_ = t.loc[:, COLS != column], t[column]\n",
    "        \n",
    "        prediction = model.predict(scaler.transform(X_))\n",
    "        visualize_prediction(y_, prediction, error_outer=ci.upper_bound, name=\"{} {}\".format(n, column))\n",
    "\n",
    "for col in COLS:\n",
    "    ridge_regression(X_train_fixed, col, test=[X_train, X_test], test_names=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"p-3 mb-2 bg-info text-white\">ARMAX Forecasting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(x_errorband, x_true):\n",
    "    return np.where(np.logical_and(x_errorband[:, 0] <= x_true, x_true <= x_errorband[:, 1]), 0, 1)\n",
    "\n",
    "def smooth(x, window_size=3, threshold=0.5):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    conv = np.convolve(x, window, 'same')\n",
    "    return np.where(conv >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoarima(trainset, column, testset=None, debug=False, pred_trust=6, err_trust=1, err_shrink=0):\n",
    "    predictions, errorbands, correlations, weights_pred, weights_err = [], [], [], [], []\n",
    "    models = {}\n",
    "    \n",
    "    # train n - 1 non-seasonal arima models, predict on test set, store result\n",
    "    for dependent in COLS:\n",
    "        if dependent == column:\n",
    "            continue\n",
    "        \n",
    "        ## TRAIN\n",
    "        ##\n",
    "        \n",
    "        X, y = trainset[dependent].values.reshape(-1, 1), trainset[column]\n",
    "        \n",
    "        # store empirical correlation and compute effective weight\n",
    "        corr = sp.stats.spearmanr(X, y)[0]\n",
    "        weight_pred = np.power(np.abs(corr), pred_trust)\n",
    "        weight_err = np.power(np.abs(corr), err_trust)\n",
    "        correlations.append(corr)\n",
    "        weights_pred.append(weight_pred)\n",
    "        weights_err.append(weight_err)\n",
    "\n",
    "        # train arima model\n",
    "        model = pm.auto_arima(y=y, exogenous=X, seasonal=False,\n",
    "                              error_action='ignore', suppress_warnings=True, stepwise=True)\n",
    "        models[dependent] = model\n",
    "    \n",
    "        ## PREDICT with one dependent feature\n",
    "        ##\n",
    "        \n",
    "        X_, y_ = testset[dependent].values.reshape(-1, 1), testset[column]\n",
    "\n",
    "        # [obsolete] append test data and predict in-sample\n",
    "        # offset = X.shape[0]\n",
    "        # model.update(y_, exogenous=X_)\n",
    "        # prediction, errorband = model.predict_in_sample(exogenous=X_, start=offset, return_conf_int=True, dynamic=True)\n",
    "        \n",
    "        prediction, errorband = model.predict(exogenous=X_, n_periods=len(testset), return_conf_int=True)\n",
    "        predictions.append(prediction)\n",
    "        errorbands.append(errorband)\n",
    "\n",
    "        if debug:\n",
    "            visualize_prediction(testset[column], prediction, error_lower=prediction - errorband[:, 0], error_upper=errorband[:, 1] - prediction,\n",
    "                                 name=\"for {} with dependent={}, correlation={:.2f}, weight_pred={:.4f}, weight_err={:.4f}\".format(column, dependent, corr, weight_pred, weight_err))\n",
    "    \n",
    "    if debug:\n",
    "        print(\"correlations\", np.abs(correlations))\n",
    "        print(\"weights_pred\", weights_pred)\n",
    "        print(\"weights_err\", weights_err)\n",
    "    \n",
    "    # unweighted average\n",
    "    # pred = np.percentile(np.vstack(predictions), q=50, axis=0)\n",
    "    # err = np.percentile(np.stack(errorbands), q=50, axis=0)\n",
    "    \n",
    "    # quantile-weighted median\n",
    "    pred = wquantiles.median(np.vstack(predictions).T, weights_pred)\n",
    "    err = wquantiles.median(np.stack(errorbands).transpose((1, 2, 0)), weights_err)\n",
    "    \n",
    "    # error shrinkage\n",
    "    iqr = err[:, 1] - err[:, 0]\n",
    "    err[:, 0] = err[:, 0] + iqr * err_shrink\n",
    "    err[:, 1] = err[:, 1] - iqr * err_shrink\n",
    "    \n",
    "    # mark outliers\n",
    "    mark = smooth(identify_outliers(err, testset[column]))\n",
    "    \n",
    "    visualize_prediction(testset[column], pred, error_lower=pred - err[:, 0], error_upper=err[:, 1] - pred, name=\"{}\".format(column), y_mark=mark, stretch=True)\n",
    "    \n",
    "    # final (merged) predictions and errorbands\n",
    "    return pred, err, mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, errs, marks = [], [], []\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(X_train_fixed)\n",
    "X_train_fixed_scaled = X_train_fixed.copy()\n",
    "X_train_fixed_scaled[:] = scaler.transform(X_train_fixed)\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[:] = scaler.transform(X_test)\n",
    "\n",
    "for col in COLS:\n",
    "    pred, err, mark = autoarima(X_train_fixed_scaled, col, testset=X_test_scaled, debug=False, pred_trust=3, err_trust=3, err_shrink=0.2)\n",
    "    \n",
    "    preds.append(pred)\n",
    "    errs.append(err)\n",
    "    marks.append(mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(marks).T\n",
    "print(\"Number of Faults {}\".format(final.sum().sum()))\n",
    "final.to_csv(r\"data/test_solution.csv\", header=False, index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
